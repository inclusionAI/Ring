diff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py
index 93d9b74d8..acde5c899 100644
--- a/vllm/engine/async_llm_engine.py
+++ b/vllm/engine/async_llm_engine.py
@@ -303,8 +303,13 @@ class _AsyncLLMEngine(LLMEngine):
             ctx.seq_group_metadata_list = seq_group_metadata_list
             ctx.scheduler_outputs = scheduler_outputs
 
-            finished_requests_ids = self.scheduler[
-                virtual_engine].get_and_reset_finished_requests_ids()
+            # finished_requests_ids = self.scheduler[
+            #     virtual_engine].get_and_reset_finished_requests_ids()
+            if not scheduler_outputs.is_empty():
+                # this will cause mamba_cache/bailing_cache failed
+                # to release finished_requests_ids of the last steps
+                finished_requests_ids = self.scheduler[
+                    virtual_engine].get_and_reset_finished_requests_ids()   
 
             # Maybe switch from async mode to sync mode
             if not allow_async_output_proc and len(ctx.output_queue) > 0:
diff --git a/vllm/model_executor/layers/lightning_attn.py b/vllm/model_executor/layers/lightning_attn.py
new file mode 100644
index 000000000..f01d77f55
--- /dev/null
+++ b/vllm/model_executor/layers/lightning_attn.py
@@ -0,0 +1,598 @@
+# SPDX-License-Identifier: Apache-2.0
+# 
+# This file is based on the original implementation from vllm:
+# https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/lightning_attn.py
+# 
+# We've made minor modifications to enhance the functionality while maintaining
+# compatibility with the original implementation. The changes include optimizations
+# for specific use cases in our deployment environment.
+
+import torch
+import triton
+import triton.language as tl
+
+@triton.jit
+def _fwd_diag_kernel(
+    Q,
+    K,
+    V,
+    Out,
+    S,
+    b: tl.constexpr,
+    h: tl.constexpr,
+    n,
+    d: tl.constexpr,
+    e: tl.constexpr,
+    BLOCK: tl.constexpr,
+    NUM_BLOCK,
+    CBLOCK: tl.constexpr,
+    NUM_CBLOCK: tl.constexpr,
+):
+    off = tl.program_id(0)
+    off_bh = off // NUM_BLOCK
+    off_block = off % NUM_BLOCK
+    off_cblock = tl.program_id(1)
+
+    off_h = off_bh % h
+
+    qk_offset = off_bh * n * d
+    v_offset = off_bh * n * e
+    o_offset = off_bh * n * e
+
+    block_offset = off_block * BLOCK
+    qk_block_offset = block_offset * d
+    v_block_offset = block_offset * e
+    o_block_offset = block_offset * e
+
+    cblock_offset = off_cblock * CBLOCK
+    q_cblock_offset = cblock_offset * d
+    o_cblock_offset = cblock_offset * e
+
+    Q_block_ptr = (Q + qk_offset + qk_block_offset + q_cblock_offset +
+                   tl.arange(0, CBLOCK)[:, None] * d +
+                   tl.arange(0, d)[None, :])
+    K_trans_block_ptr = (K + qk_offset + qk_block_offset +
+                         tl.arange(0, CBLOCK)[None, :] * d +
+                         tl.arange(0, d)[:, None])
+    V_block_ptr = (V + v_offset + v_block_offset +
+                   tl.arange(0, CBLOCK)[:, None] * e +
+                   tl.arange(0, e)[None, :])
+    O_block_ptr = (Out + o_offset + o_block_offset + o_cblock_offset +
+                   tl.arange(0, CBLOCK)[:, None] * e +
+                   tl.arange(0, e)[None, :])
+
+    S_block_ptr = S + off_h
+    s = tl.load(S_block_ptr)
+
+    i = off_cblock
+    q_index = tl.arange(0, CBLOCK) + i * CBLOCK
+
+    q = tl.load(Q_block_ptr,
+                mask=block_offset + q_index[:, None] < n,
+                other=0.0).to(tl.float32)
+
+    qkv = tl.zeros([CBLOCK, e], dtype=tl.float32)
+    # none diag
+
+    for j in range(i + 1):
+        kv_index = tl.arange(0, CBLOCK) + j * CBLOCK
+        diff = q_index[:, None] - kv_index[None, :]
+        s_index = s * diff
+        s_index = tl.where(diff >= 0, -s_index, float("-inf"))
+        decay = tl.exp(s_index)
+
+        k_trans = tl.load(
+            K_trans_block_ptr,
+            mask=block_offset + kv_index[None, :] < n,
+            other=0.0,
+        ).to(tl.float32)
+        v = tl.load(
+            V_block_ptr,
+            mask=block_offset + kv_index[:, None] < n,
+            other=0.0,
+        ).to(tl.float32)
+
+        qk = tl.dot(q, k_trans) * decay
+
+        qkv += tl.dot(qk, v)
+
+        K_trans_block_ptr += CBLOCK * d
+        V_block_ptr += CBLOCK * e
+
+    tl.store(
+        O_block_ptr,
+        qkv.to(O_block_ptr.dtype.element_ty),
+        mask=block_offset + q_index[:, None] < n,
+    )
+
+
+@triton.jit
+def _fwd_kv_parallel(
+    K,
+    V,
+    K_decay,
+    KV,
+    b: tl.constexpr,
+    h: tl.constexpr,
+    n,
+    d: tl.constexpr,
+    e: tl.constexpr,
+    BLOCK: tl.constexpr,
+    NUM_BLOCK,
+    D_FBLOCK: tl.constexpr,
+    E_FBLOCK: tl.constexpr,
+    NUM_FBLOCK: tl.constexpr,
+    CBLOCK: tl.constexpr,
+    NUM_CBLOCK: tl.constexpr,
+):
+    off_bh = tl.program_id(0)
+    off_block = tl.program_id(1)
+
+    off_h = off_bh % h
+
+    block_offset = off_block * BLOCK
+
+    k_block_offset = block_offset * d
+    v_block_offset = block_offset * e
+    kv_block_offset = off_block * d * e
+
+    k_offset = off_bh * n * d
+    v_offset = off_bh * n * e
+    kv_offset = off_bh * NUM_BLOCK * d * e
+
+    # (CBLOCK, FBLOCK)
+    K_trans_block_ptr = (
+        K + k_offset + k_block_offset +
+        tl.arange(0, CBLOCK)[None, :] * d  # d x c
+        + tl.arange(0, D_FBLOCK)[:, None])
+    V_block_ptr = (
+        V + v_offset + v_block_offset +
+        tl.arange(0, CBLOCK)[:, None] * e  # c x d
+        + tl.arange(0, E_FBLOCK)[None, :])
+    KV_block_ptr = (KV + kv_offset + kv_block_offset +
+                    tl.arange(0, D_FBLOCK)[:, None] * e +
+                    tl.arange(0, E_FBLOCK)[None, :])
+
+    k_decay_ptr = (K_decay + off_h * BLOCK + tl.arange(0, CBLOCK)[None, :])
+
+    # compute block array
+    kv_index = tl.arange(0, CBLOCK)
+
+    kv = tl.zeros([D_FBLOCK, E_FBLOCK], dtype=tl.float32)
+
+    if off_block == NUM_BLOCK - 1:
+        split_n = n - (NUM_BLOCK - 1) * BLOCK
+    else:
+        split_n = BLOCK
+    left_shift = tl.cdiv(split_n, CBLOCK) * CBLOCK - split_n
+    num_blocks = min(tl.cdiv(split_n, CBLOCK), NUM_CBLOCK)
+    k_decay_ptr += (NUM_CBLOCK - num_blocks) * CBLOCK
+    for j in range(num_blocks):
+        # right align k, v with CBLOCK
+        left_bound = (1 - j) * left_shift
+        k_trans = tl.load(K_trans_block_ptr - left_shift * d,
+                          mask=kv_index[None, :] >= left_bound,
+                          other=0.0)
+        v = tl.load(V_block_ptr - left_shift * d,
+                    mask=kv_index[:, None] >= left_bound,
+                    other=0.0).to(tl.float32)
+
+
+        k_decay = tl.load(k_decay_ptr)
+        kv += tl.dot(k_trans * k_decay, v)
+
+        K_trans_block_ptr += CBLOCK * d
+        V_block_ptr += CBLOCK * e
+        k_decay_ptr += CBLOCK
+
+    tl.store(KV_block_ptr, kv.to(KV_block_ptr.dtype.element_ty))
+
+
+@triton.jit
+def _fwd_kv_reduce(
+    K,
+    V,
+    S,
+    KV,
+    KV_HISTORY,
+    b: tl.constexpr,
+    h: tl.constexpr,
+    n,
+    d: tl.constexpr,
+    e: tl.constexpr,
+    BLOCK: tl.constexpr,
+    NUM_BLOCK,
+    D_FBLOCK: tl.constexpr,
+    E_FBLOCK: tl.constexpr,
+    NUM_FBLOCK: tl.constexpr,
+    CBLOCK: tl.constexpr,
+    NUM_CBLOCK: tl.constexpr,
+):
+    off_bh = tl.program_id(0)
+    off_h = off_bh % h
+
+    kv_offset = off_bh * NUM_BLOCK * d * e
+
+    # (CBLOCK, FBLOCK)
+    KV_block_ptr = (KV + kv_offset + tl.arange(0, D_FBLOCK)[:, None] * e +
+                    tl.arange(0, E_FBLOCK)[None, :])
+
+    s_ptrs = S + off_h
+    s = tl.load(s_ptrs)
+
+    # Initialize kv from KV_HISTORY
+    kv_history_offset = off_bh * d * e
+    KV_HISTORY_block_ptr = (KV_HISTORY + kv_history_offset +
+                            tl.arange(0, D_FBLOCK)[:, None] * e +
+                            tl.arange(0, E_FBLOCK)[None, :])
+    # compute block array
+    # last step
+    kv_pre = tl.load(KV_HISTORY_block_ptr).to(tl.float32)
+    for i in range(NUM_BLOCK):
+        block_size = min(n - i * BLOCK, BLOCK)
+        block_decay = tl.exp(-s.to(tl.float32) * block_size)
+
+        kv_cur = tl.load(KV_block_ptr).to(tl.float32)
+        tl.store(KV_block_ptr, kv_pre.to(KV_block_ptr.dtype.element_ty))
+
+        kv_pre = block_decay * kv_pre + kv_cur
+        KV_block_ptr += d * e
+    tl.store(KV_HISTORY_block_ptr, kv_pre)
+
+
+@triton.jit
+def _fwd_none_diag_kernel(
+    Q,
+    K,
+    V,
+    Out,
+    S,
+    KV,
+    b: tl.constexpr,
+    h: tl.constexpr,
+    n,
+    d: tl.constexpr,
+    e: tl.constexpr,
+    BLOCK: tl.constexpr,
+    NUM_BLOCK,
+    D_FBLOCK: tl.constexpr,
+    E_FBLOCK: tl.constexpr,
+    NUM_FBLOCK: tl.constexpr,
+    CBLOCK: tl.constexpr,
+    NUM_CBLOCK: tl.constexpr,
+):
+    off_bh = tl.program_id(0)
+    off_h = off_bh % h
+
+    off_nc = tl.program_id(1)
+    off_n = off_nc // NUM_CBLOCK
+    off_c = off_nc % NUM_CBLOCK
+    off_e = tl.program_id(2)
+
+    n_offset = off_n * BLOCK
+    c_offset = off_c * CBLOCK
+    e_offset = off_e * E_FBLOCK
+    block_offset = n_offset + c_offset
+
+    q_offset = off_bh * n * d + (n_offset + c_offset) * d
+    o_offset = off_bh * n * e + (n_offset + c_offset) * e + e_offset
+
+    kv_offset = off_bh * NUM_BLOCK * d * e + off_n * d * e + e_offset
+
+    Q_block_ptr = (Q + q_offset + tl.arange(0, CBLOCK)[:, None] * d +
+                   tl.arange(0, d)[None, :])
+    O_block_ptr = (Out + o_offset + tl.arange(0, CBLOCK)[:, None] * e +
+                   tl.arange(0, E_FBLOCK)[None, :])
+    KV_block_ptr = (KV + kv_offset + tl.arange(0, d)[:, None] * e +
+                    tl.arange(0, E_FBLOCK)[None, :])
+    S_block_ptr = S + off_h
+    s = tl.load(S_block_ptr)
+
+    c_array = tl.arange(0, CBLOCK)
+
+    kv = tl.load(KV_block_ptr).to(tl.float32)
+    q_index = block_offset + tl.arange(0, CBLOCK)
+    q = tl.load(Q_block_ptr, mask=q_index[:, None] < n,
+                other=0.).to(tl.float32)
+
+    q_decay = tl.exp(-s.to(tl.float32) * (off_c * CBLOCK + c_array[:, None]))
+    qkv_none_diag = tl.dot(q, kv) * q_decay
+
+    qkv_diag = tl.load(O_block_ptr, mask=q_index[:, None] < n,
+                       other=0.).to(tl.float32)
+
+    qkv = qkv_diag + qkv_none_diag
+
+    tl.store(O_block_ptr,
+             qkv.to(O_block_ptr.dtype.element_ty),
+             mask=q_index[:, None] < n)
+
+
+class _attention(torch.autograd.Function):
+
+    @staticmethod
+    def forward(ctx, q, k, v, s, kv_history):
+        q = q.contiguous()
+        k = k.contiguous()
+        v = v.contiguous()
+        s = s.contiguous()
+        # only support for Ampere now
+        capability = torch.cuda.get_device_capability()
+        if capability[0] < 8:
+            raise RuntimeError("Flash attention currently only supported",
+                               "for compute capability >= 80")
+        # shape constraints
+        b, h, n, d = q.shape
+        e = v.shape[-1]
+        # right
+        o = torch.empty((b, h, n, e), dtype=q.dtype, device=q.device)
+
+        BLOCK = 256
+        NUM_BLOCK = triton.cdiv(n, BLOCK)
+
+        CBLOCK = 64
+        CBLOCK = 32
+        NUM_CBLOCK = BLOCK // CBLOCK
+        assert BLOCK % CBLOCK == 0, "BLOCK must be a multiple of CBLOCK"
+
+        array = torch.arange(0, BLOCK, device=q.device) + 1
+        k_decay = torch.exp(-s * (BLOCK - array.reshape(1, -1)))
+
+        grid = (b * h * NUM_BLOCK, NUM_CBLOCK)
+        _fwd_diag_kernel[grid](
+            q,
+            k,
+            v,
+            o,
+            s,
+            b,
+            h,
+            n,
+            d,
+            e,
+            BLOCK=BLOCK,
+            NUM_BLOCK=NUM_BLOCK,
+            CBLOCK=CBLOCK,
+            NUM_CBLOCK=NUM_CBLOCK,
+        )
+
+        NUM_FBLOCK = 1
+        D_FBLOCK = d // NUM_FBLOCK
+        assert d % NUM_FBLOCK == 0
+        E_FBLOCK = e // NUM_FBLOCK
+        assert e % NUM_FBLOCK == 0
+
+        CBLOCK = 64
+        NUM_CBLOCK = BLOCK // CBLOCK
+        assert BLOCK % CBLOCK == 0, "BLOCK must be a multiple of CBLOCK"
+
+        kv = torch.empty((b, h, NUM_BLOCK, d, e),
+                         dtype=torch.float32,
+                         device=q.device)
+        grid = (b * h, NUM_BLOCK)
+        _fwd_kv_parallel[grid](
+            k,
+            v,
+            k_decay,
+            kv,
+            b,
+            h,
+            n,
+            d,
+            e,
+            BLOCK=BLOCK,
+            NUM_BLOCK=NUM_BLOCK,
+            D_FBLOCK=D_FBLOCK,
+            E_FBLOCK=E_FBLOCK,
+            NUM_FBLOCK=NUM_FBLOCK,
+            CBLOCK=CBLOCK,
+            NUM_CBLOCK=NUM_CBLOCK,
+        )
+
+        grid = (b * h, NUM_FBLOCK)
+        _fwd_kv_reduce[grid](
+            k,
+            v,
+            s,
+            kv,
+            kv_history,
+            b,
+            h,
+            n,
+            d,
+            e,
+            BLOCK=BLOCK,
+            NUM_BLOCK=NUM_BLOCK,
+            D_FBLOCK=D_FBLOCK,
+            E_FBLOCK=E_FBLOCK,
+            NUM_FBLOCK=NUM_FBLOCK,
+            CBLOCK=CBLOCK,
+            NUM_CBLOCK=NUM_CBLOCK,
+        )
+
+        grid = (b * h, NUM_BLOCK * NUM_CBLOCK)
+        _fwd_none_diag_kernel[grid](
+            q,
+            k,
+            v,
+            o,
+            s,
+            kv,
+            b,
+            h,
+            n,
+            d,
+            e,
+            BLOCK=BLOCK,
+            NUM_BLOCK=NUM_BLOCK,
+            D_FBLOCK=D_FBLOCK,
+            E_FBLOCK=E_FBLOCK,
+            NUM_FBLOCK=NUM_FBLOCK,
+            CBLOCK=CBLOCK,
+            NUM_CBLOCK=NUM_CBLOCK,
+        )
+
+        ctx.save_for_backward(q, k, v, s, kv)
+        ctx.BLOCK = BLOCK
+
+        return o, torch.cat([kv, kv_history.unsqueeze(2)], dim=2)
+
+lightning_attention_ = _attention.apply
+
+
+def lightning_attention(q, k, v, ed, block_size=256, kv_history=None):
+    d = q.shape[-1]
+    e = v.shape[-1]
+    m = 128 if d >= 128 else 64
+    arr = [m * i for i in range(d // m + 1)]
+    if arr[-1] != d:
+        arr.append(d)
+    n = len(arr)
+    output = 0
+    if kv_history is None:
+        kv_history = torch.zeros((q.shape[0], q.shape[1], d, e),
+                                 dtype=torch.float32,
+                                 device=q.device)
+    else:
+        # make sure run in functional programming style
+        kv_history = kv_history.clone().contiguous()
+
+    for i in range(n - 1):
+        s = arr[i]
+        e = arr[i + 1]
+        q1 = q[..., s:e]
+        k1 = k[..., s:e]
+        o, kv = lightning_attention_(q1, k1, v, ed, kv_history)
+        output = output + o
+    return output, kv
+
+
+def lightning_attention2_parallel(q,
+                                  k,
+                                  v,
+                                  ed,
+                                  block_size=256,
+                                  kv_history=None):
+    return lightning_attention(q, k, v, ed, block_size, kv_history)
+
+
+@triton.jit
+def _linear_attn_decode_kernel(
+    # Pointers to matrices
+    q_ptr,
+    k_ptr,
+    v_ptr,  # [B, H, 1, D]  
+    kv_cache_ptr,  # [B, H, D, D] 
+    slope_rate,
+    slot_idx,
+    output_ptr,  # [B, H, 1, D]
+    B,
+    H,
+    D: tl.constexpr,
+    # Matrix dimensions
+    qkv_b_stride,
+    qkv_h_stride,
+    cache_b_stride,
+    cache_h_stride,
+    cache_d0_stride,
+    cache_d1_stride,
+    BLOCK_SIZE: tl.constexpr,
+):
+
+    pid_b = tl.program_id(0)
+    pid_h = tl.program_id(1)
+    pid_d = tl.program_id(2)
+
+    slot_id = tl.load(slot_idx + pid_b)
+
+    # return when padding
+    if slot_id == -1:
+        return
+
+    batch_id = pid_b
+    head_id = pid_h
+
+    ratio = tl.load(slope_rate + pid_h).to(tl.float32)
+
+    qk_d_offsets = tl.arange(0, D)
+    v_d_offsets = tl.arange(0, BLOCK_SIZE) + pid_d * BLOCK_SIZE
+    cache_d_offsets = qk_d_offsets[:, None] * cache_d0_stride + v_d_offsets[
+        None, :] * cache_d1_stride
+
+    q_offset = batch_id * qkv_b_stride + head_id * qkv_h_stride
+    k_offset = batch_id * qkv_b_stride + head_id * qkv_h_stride
+    v_offset = batch_id * qkv_b_stride + head_id * qkv_h_stride
+
+    cache_offset = slot_id * cache_b_stride + head_id * cache_h_stride
+
+    qk_mask = qk_d_offsets < D
+    v_mask = v_d_offsets < D
+    # load data to shm
+    q = tl.load(q_ptr + q_offset + qk_d_offsets, mask=qk_mask, other=0.0)
+    k = tl.load(k_ptr + k_offset + qk_d_offsets, mask=qk_mask, other=0.0)
+    v = tl.load(v_ptr + v_offset + v_d_offsets, mask=v_mask, other=0.0)
+
+    kv_outer = k[:, None] * v[None, :]  # [D, BLOCK_SIZE]
+    kv_mask = qk_mask[:, None] & v_mask[None, :]
+
+    # compute decay
+    ratio = tl.exp(-ratio)
+    # load kv_cache
+    kv_ptr = kv_cache_ptr + cache_offset + cache_d_offsets
+    kv_cache_old = tl.load(kv_ptr, mask=kv_mask, other=0.0)
+    kv_outer = kv_outer + ratio * kv_cache_old
+
+    output = q[:, None].to(tl.float32) * kv_outer
+    output = tl.sum(output, axis=0)
+
+    tl.store(kv_ptr, kv_outer, mask=kv_mask)
+    tl.store(output_ptr + q_offset + v_d_offsets, output, mask=v_mask)
+
+
+def linear_decode_forward_triton(
+    q: torch.Tensor,  # [B, H, 1, D] 
+    k: torch.Tensor,  # [B, H, 1, D]
+    v: torch.Tensor,  # [B, H, 1, D] 
+    kv_caches: torch.Tensor,  # [B, H, D, D]
+    slope_rate: torch.Tensor,  # float
+    slot_idx: torch.Tensor,
+    BLOCK_SIZE: int = 32,
+) -> torch.Tensor:
+
+    B, H, _, D = q.shape
+    assert k.shape == (B, H, 1, D)
+    assert v.shape == (B, H, 1, D)
+
+    output = torch.empty_like(q)
+
+    grid = (B, H, D // BLOCK_SIZE)
+
+    qkv_b_stride = q.stride(0)
+    qkv_h_stride = q.stride(1)
+
+    cache_b_stride = kv_caches.stride(0)
+    cache_h_stride = kv_caches.stride(1)
+    cache_d0_stride = kv_caches.stride(2)
+    cache_d1_stride = kv_caches.stride(3)
+
+    # launch kernel
+    _linear_attn_decode_kernel[grid](
+        q,
+        k,
+        v,
+        kv_caches,
+        slope_rate,
+        slot_idx,
+        output,
+        B,
+        H,
+        D,
+        qkv_b_stride,
+        qkv_h_stride,
+        cache_b_stride,
+        cache_h_stride,
+        cache_d0_stride,
+        cache_d1_stride,
+        BLOCK_SIZE=BLOCK_SIZE,
+    )
+    return output.view(B, -1)
diff --git a/vllm/model_executor/models/bailing_linear_cache.py b/vllm/model_executor/models/bailing_linear_cache.py
new file mode 100644
index 000000000..af5294770
--- /dev/null
+++ b/vllm/model_executor/models/bailing_linear_cache.py
@@ -0,0 +1,43 @@
+# SPDX-License-Identifier: Apache-2.0
+#
+# This file is adapted from the original implementation in vllm:
+# https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/minimax_cache.py
+#
+# We've renamed the components from "Minimax" to "Bailing" to better reflect our use case,
+# while maintaining the same functionality and interface as the original implementation.
+# The core logic remains unchanged.
+
+from dataclasses import dataclass
+
+import torch
+
+from vllm.model_executor.models.constant_size_cache import ConstantSizeCache
+
+
+@dataclass
+class BailingCacheParams:
+    bailing_cache: torch.Tensor = torch.Tensor()
+    state_indices_tensor: torch.Tensor = torch.Tensor()
+
+    def at_layer_idx(self, layer_idx):
+        return BailingCacheParams(self.bailing_cache[layer_idx, ...],
+                                  self.state_indices_tensor)
+
+
+class BailingCacheManager(ConstantSizeCache):
+
+    def __init__(self, dtype, cache_shape):
+        super().__init__(cache_shape[1])  # max_batch_size is cache_shape[1]
+        self._bailing_cache = torch.empty(size=cache_shape,
+                                          dtype=dtype,
+                                          device="cuda")
+
+    @property
+    def cache(self):
+        return self._bailing_cache
+
+    def _copy_cache(self, from_index: int, to_index: int):
+        assert len(self.cache) > 0
+        for cache_t in self.cache:
+            cache_t[:, to_index].copy_(cache_t[:, from_index],
+                                       non_blocking=True)
diff --git a/vllm/model_executor/models/bailing_moe.py b/vllm/model_executor/models/bailing_moe.py
new file mode 100644
index 000000000..774580c94
--- /dev/null
+++ b/vllm/model_executor/models/bailing_moe.py
@@ -0,0 +1,535 @@
+# coding=utf-8
+""" PyTorch Bailing model. """
+
+from typing import Iterable, List, Optional, Tuple, Union, Set
+
+import torch
+from torch import nn
+
+from vllm.model_executor.layers.activation import get_act_fn, SiluAndMul
+from vllm.attention import Attention, AttentionMetadata
+from vllm.config import CacheConfig, VllmConfig
+from vllm.model_executor.layers.fused_moe import fused_moe, FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (ColumnParallelLinear,
+                                               MergedColumnParallelLinear,
+                                               ReplicatedLinear,
+                                               QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig)
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import Sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.distributed import (get_pp_group,
+                              get_tensor_model_parallel_rank,
+                              get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.utils import set_weight_attrs
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.sequence import IntermediateTensors
+from vllm.transformers_utils.configs.bailing_moe import BailingMoeConfig
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.config import LoRAConfig
+
+from .interfaces import SupportsLoRA, SupportsPP
+from .utils import (PPMissingLayer,
+                    is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory,
+                    make_layers,
+                    maybe_prefix)
+
+KVCache = Tuple[torch.Tensor, torch.Tensor]
+
+
+class BailingAttention(nn.Module):
+
+    def __init__(
+            self,
+            config: BailingMoeConfig,
+            cache_config: Optional[CacheConfig] = None,
+            quant_config: Optional[QuantizationConfig] = None,
+            prefix: str = "",
+    ):
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        self.total_num_heads = config.num_attention_heads
+        self.total_kv_heads = config.num_key_value_heads
+        tp_size = get_tensor_model_parallel_world_size()
+
+        assert self.total_num_heads % tp_size == 0
+        assert self.total_kv_heads % tp_size == 0
+        assert self.total_num_heads >= self.total_kv_heads
+
+        self.num_heads = self.total_num_heads // tp_size
+        self.head_dim = config.head_dim or (self.hidden_size // self.total_num_heads)
+        self.q_size_per_rank = self.head_dim * self.num_heads
+
+        self.num_kv_heads = self.total_kv_heads // tp_size
+        self.kv_size_per_rank = self.num_kv_heads * self.head_dim
+
+        self.scale = self.head_dim ** -0.5
+
+        self.query_key_value = QKVParallelLinear(
+            self.hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_kv_heads,
+            bias=(config.use_bias or config.use_qkv_bias),
+            quant_config=quant_config,
+            prefix=f"{prefix}.query_key_value",
+        )
+
+        self.dense = RowParallelLinear(self.total_num_heads * self.head_dim,
+                                       self.hidden_size,
+                                       bias=config.use_bias,
+                                       quant_config=quant_config,
+                                       prefix=f"{prefix}.dense",)
+
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scale,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              prefix=f"{prefix}.attn")
+
+
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=config.max_position_embeddings,
+            base=config.rope_theta,
+            is_neox_style=True,
+            rope_scaling=config.rope_scaling,
+        )
+
+    def forward(
+            self,
+            hidden_states: torch.Tensor,
+            position_ids: torch.Tensor,
+            kv_cache: KVCache,
+            attn_metadata: AttentionMetadata,
+    ) -> torch.Tensor:
+
+        qkv, _ = self.query_key_value(hidden_states)
+        q, k, v = qkv.split(
+            [self.q_size_per_rank, self.kv_size_per_rank, self.kv_size_per_rank],
+            dim=-1
+        )
+
+
+        q, k = self.rotary_emb(position_ids, q, k)
+
+        context_layer = self.attn(
+            q,
+            k,
+            v,
+            kv_cache,
+            attn_metadata,
+        )
+
+        attn_output, _ = self.dense(context_layer)
+        return attn_output
+
+
+class BailingMLP(nn.Module):
+
+    def __init__(
+            self,
+            intermediate_size: int,
+            config: BailingMoeConfig,
+            quant_config: Optional[QuantizationConfig] = None,
+            reduce_results: Optional[bool] = True,
+            prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            config.hidden_size, [intermediate_size] * 2,
+            bias=config.use_bias,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj",
+        )
+        self.down_proj = RowParallelLinear(
+            intermediate_size,
+            config.hidden_size,
+            bias=config.use_bias,
+            quant_config=quant_config,
+            reduce_results=reduce_results,
+            prefix=f"{prefix}.down_proj",
+        )
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x):
+        x, _ = self.gate_up_proj(x)
+        x = self.act_fn(x)
+        x, _ = self.down_proj(x)
+        return x
+
+class BailingMoE(nn.Module):
+
+    def __init__(
+            self,
+            intermediate_size: int,
+            config: BailingMoeConfig,
+            quant_config: Optional[QuantizationConfig] = None,
+            reduce_results: Optional[bool] = True,
+            prefix: str = "",
+    ):
+        super().__init__()
+
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.tp_rank = get_tensor_model_parallel_rank()
+        self.num_experts = config.num_experts
+        self.top_k = config.num_experts_per_tok
+        self.norm_expert_prob = config.norm_topk_prob
+        self.hidden_size = config.hidden_size
+        self.quant_config = quant_config
+        self.num_shared_experts = config.num_shared_experts
+        # Gate always runs at half / full precision for now.
+        self.gate = ReplicatedLinear(self.hidden_size,
+                                     self.num_experts,
+                                     bias=False,
+                                     quant_config=None)
+
+        self.experts = FusedMoE(
+            num_experts=self.num_experts,
+            top_k=self.top_k,
+            hidden_size=self.hidden_size,
+            intermediate_size=config.moe_intermediate_size,
+            reduce_results=False,
+            renormalize=self.norm_expert_prob,
+            quant_config=quant_config,
+            prefix=f"{prefix}.experts"
+        )
+
+        if self.num_shared_experts > 0:
+            intermediate_size = (config.moe_intermediate_size *
+                                 self.num_shared_experts)
+            self.shared_experts = BailingMLP(
+                intermediate_size=intermediate_size,
+                config=config,
+                quant_config=quant_config,
+                reduce_results=False,
+                prefix=f"{prefix}.shared_experts"
+            )
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        num_tokens, hidden_size = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_size)
+        if self.num_shared_experts > 0:
+            shared_output = self.shared_experts(hidden_states)
+        # router_logits: (num_tokens, n_experts)
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.experts(
+            hidden_states=hidden_states, router_logits=router_logits
+        )
+
+        if self.num_shared_experts > 0:
+            final_hidden_states = final_hidden_states + shared_output
+
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+        return final_hidden_states.view(num_tokens, hidden_size)
+
+class BailingMoeBlock(nn.Module):
+
+    def __init__(
+            self,
+            config: BailingMoeConfig,
+            cache_config: Optional[CacheConfig] = None,
+            quant_config: Optional[QuantizationConfig] = None,
+            prefix: str = "",
+    ):
+        super().__init__()
+        hidden_size = config.hidden_size
+        intermediate_size = config.intermediate_size
+        self.input_layernorm = RMSNorm(hidden_size, eps=config.rms_norm_eps)
+        self.attention = BailingAttention(config,
+                                      cache_config,
+                                      quant_config,
+                                      prefix=f"{prefix}.attention")
+        self.post_attention_layernorm = RMSNorm(hidden_size, eps=config.rms_norm_eps)
+        self.mlp = BailingMoE(intermediate_size, config, quant_config, True, prefix=f"{prefix}.mlp")
+
+    def forward(
+            self,
+            hidden_states: torch.Tensor,
+            position_ids: torch.Tensor,
+            kv_cache: KVCache,
+            attn_metadata: AttentionMetadata,
+            residual: Optional[torch.Tensor],
+    ) -> torch.Tensor:
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+
+        hidden_states = self.attention(
+            hidden_states=hidden_states,
+            position_ids=position_ids,
+            kv_cache=kv_cache,
+            attn_metadata=attn_metadata
+        )
+
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+class BailingMoeModel(nn.Module):
+
+    def __init__(
+            self,
+            *, 
+            vllm_config: VllmConfig,
+            prefix: str = "",
+    ):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        self.config = config
+        self.vocab_size = config.vocab_size
+        self.embed_dim = config.hidden_size
+
+        if get_pp_group().is_first_rank or (config.tie_word_embeddings
+                                            and get_pp_group().is_last_rank):
+            self.word_embeddings = VocabParallelEmbedding(self.vocab_size, self.embed_dim)
+        else:
+            self.word_embeddings = PPMissingLayer()
+
+        self.embedding_dropout = torch.nn.Dropout(config.embedding_dropout)
+
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: BailingMoeBlock(
+                config=config,
+                cache_config=cache_config,
+                quant_config=quant_config,
+                prefix=prefix,
+            ),
+            prefix=f"{prefix}.layers"
+        )
+
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size
+            )
+        )
+
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(self.embed_dim, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+    
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.word_embeddings(input_ids)
+
+    def forward(
+            self,
+            input_ids: torch.Tensor,
+            position_ids: torch.Tensor,
+            kv_caches: List[KVCache],
+            attn_metadata: AttentionMetadata,
+            intermediate_tensors: Optional[IntermediateTensors],
+            inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+
+        for i in range(self.start_layer, self.end_layer):
+            layer = self.layers[i]
+            hidden_states, residual = layer(
+                hidden_states,
+                position_ids,
+                kv_caches[i - self.start_layer],
+                attn_metadata,
+                residual
+            )
+
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+
+class BailingMoeForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
+
+    packed_modules_mapping = {
+        "query_key_value": ["query_key_value"],
+        "dense_h_to_4h": ["dense_h_to_4h"],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    # LoRA specific attributes
+    supported_lora_modules = [
+        "query_key_value",
+        "dense",
+        "dense_h_to_4h",
+        "dense_4h_to_h",
+        "gate_up_proj",
+        "down_proj",
+    ]
+    embedding_modules = {}
+    embedding_padding_modules = []
+
+    def __init__(
+            self,
+            *,
+            vllm_config: VllmConfig,
+            prefix: str = "",
+    ) -> None:
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.lora_config = lora_config
+        self.quant_config = quant_config
+        self.max_position_embeddings = config.max_position_embeddings
+        self.model = BailingMoeModel(
+                                    vllm_config=vllm_config,
+                                    prefix=maybe_prefix(prefix, "model")
+        )
+        if get_pp_group().is_last_rank:
+            self.lm_head = self.word_embeddings if config.tie_word_embeddings \
+                else ParallelLMHead(config.vocab_size, config.hidden_size, quant_config=quant_config)
+            self.logits_processor = LogitsProcessor(config.vocab_size)
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.sampler = get_sampler()
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors
+        )
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+            self,
+            input_ids: torch.Tensor,
+            positions: torch.Tensor,
+            kv_caches: List[KVCache],
+            attn_metadata: AttentionMetadata,
+            intermediate_tensors: Optional[IntermediateTensors] = None,
+            inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        model_output = self.model(input_ids, positions, kv_caches,
+                                         attn_metadata, intermediate_tensors,
+                                         inputs_embeds)
+        return model_output
+
+    def compute_logits(
+            self,
+            hidden_states: torch.Tensor,
+            sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states, sampling_metadata)
+        return logits
+
+    def sample(
+            self,
+            logits: torch.Tensor,
+            sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]) -> Set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.num_experts)
+
+        params_dict = dict(self.named_parameters(remove_duplicate=False))
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+            if (("v_head" in name) or ("inv_freq" in name) or
+                    (self.config.tie_word_embeddings and "lm_head" in name)):
+                continue
+            if self.config.norm_head and "lm_head.weight" in name:
+                import torch.nn.functional as F
+                loaded_weight = F.normalize(loaded_weight, dim=0, p=2, eps=1e-7)
+
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                if "mlp.experts" in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if name not in params_dict:
+                    continue
+
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                  loaded_weight,
+                                  name,
+                                  shard_id=shard_id,
+                                  expert_id=expert_id)
+                    break
+                else:
+                    if name.endswith(".bias") and name not in params_dict:
+                        continue
+                    if name not in params_dict:
+                        continue
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader", default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
diff --git a/vllm/model_executor/models/bailing_moe_linear.py b/vllm/model_executor/models/bailing_moe_linear.py
new file mode 100644
index 000000000..78f57a184
--- /dev/null
+++ b/vllm/model_executor/models/bailing_moe_linear.py
@@ -0,0 +1,1118 @@
+# coding=utf-8
+""" PyTorch BailingMoeLinear model. """
+
+from typing import Iterable, List, Optional, Tuple, Union, Set, Callable, Dict
+import math
+import torch
+from torch import nn
+import torch.nn.functional as F
+
+from vllm.model_executor.layers.activation import get_act_fn, SiluAndMul
+from vllm.attention import Attention, AttentionMetadata
+from vllm.config import CacheConfig, VllmConfig
+from vllm.model_executor.custom_op import CustomOp
+from vllm.forward_context import get_forward_context
+from vllm.model_executor.layers.fused_moe import fused_moe, FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (ColumnParallelLinear,
+                                               MergedColumnParallelLinear,
+                                               ReplicatedLinear,
+                                               QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig)
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.lightning_attn import (
+    lightning_attention2_parallel, linear_decode_forward_triton)
+from .bailing_linear_cache import BailingCacheManager, BailingCacheParams
+from vllm.model_executor.layers.sampler import Sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.distributed import (get_pp_group,
+                              get_tensor_model_parallel_rank,
+                              get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.utils import set_weight_attrs
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.sequence import IntermediateTensors
+from vllm.transformers_utils.configs.bailing_moe_linear import BailingMoeLinearConfig
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.config import LoRAConfig
+
+from .interfaces import SupportsLoRA, SupportsPP, HasInnerState
+from .utils import (PPMissingLayer,
+                    is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory,
+                    make_layers,
+                    maybe_prefix)
+
+KVCache = Tuple[torch.Tensor, torch.Tensor]
+
+def is_linear_layer(layer_idx, layer_group_size=7):
+    if layer_idx is None:
+        return False
+    return (layer_idx + 1) % layer_group_size != 0
+
+def replace_weight_name(name: str,
+                        key: str = None,
+                        to: str = None,
+                        count: int = None,
+                        prefix: str = None) -> str:
+    name = name.replace(key, to) if count is None else \
+        name.replace(key, to, count)
+    return name
+
+def weight_loader_with_alias(alias: str):
+
+    def wrapper(func: Callable):
+
+        def inner_func(param: torch.Tensor,
+                       loaded_weight: torch.Tensor,
+                       *args,
+                       prefix: str = None,
+                       **kwargs):
+            # pf = "[vLLM][load]" + " " if prefix is None else f"[{prefix}] "
+            value = func(param, loaded_weight, *args, **kwargs)
+            return value
+
+        return inner_func
+
+    return wrapper
+
+class BailingRMSNormTP(CustomOp):
+    """
+    Tensor-Parallel RMSNorm implementation for Bailing models.
+    
+    This class is adapted from MiniMaxText01RMSNormTP in vllm:
+    https://github.com/vllm-project/vllm/blob/a9138e85b14047e06300685b48e3485b995425fb/vllm/model_executor/models/minimax_text_01.py#L75
+    
+    The implementation maintains the same functionality while being renamed to
+    match our Bailing model naming convention.
+    """    
+    name = "BailingRMSNormTP"
+
+    def __init__(self, hidden_size: int, eps: float = 1e-6) -> None:
+        super().__init__()
+        self.tp_world = get_tensor_model_parallel_world_size()
+        self.tp_rank = get_tensor_model_parallel_rank()
+        self.weight = nn.Parameter(torch.ones(int(hidden_size /
+                                                  self.tp_world)))
+
+        self.weight.weight_loader = self.weight_loader
+        self.variance_epsilon = eps
+        return
+
+    @staticmethod
+    def weight_loader(
+        param: nn.Parameter,
+        loaded_weight: torch.Tensor,
+    ) -> None:
+        tp_world = get_tensor_model_parallel_world_size()
+        tp_rank = get_tensor_model_parallel_rank()
+
+        shard_size = loaded_weight.shape[0] // tp_world
+        shard = slice(tp_rank * shard_size, (tp_rank + 1) * shard_size)
+        param.data.copy_(loaded_weight[shard])
+        return
+
+    @staticmethod
+    def weight2param_match(
+        model: nn.Module,
+        name: str,
+        all_params: Dict[str, torch.Tensor],
+    ) -> bool:
+        return bool(name in all_params and "norm" in name
+                    and not name.endswith(".bias"))
+
+    @staticmethod
+    def weight2param_copy(
+        model: nn.Module,
+        name: str,
+        all_params: Dict[str, torch.Tensor],
+        loaded_weight: torch.Tensor,
+        prefix: str = "norm",
+    ) -> None:
+        name = replace_weight_name(name, prefix=prefix)
+        param = all_params[name]
+        if is_pp_missing_parameter(name, model):
+            return
+        loader = getattr(param, "weight_loader",
+                         BailingRMSNormTP.weight_loader)
+        loader = weight_loader_with_alias(name)(loader)
+        loader(param, loaded_weight)
+        return
+    
+    def forward(
+        self,
+        x: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
+        assert residual is None, "RMSNorm does not support residual connection."
+        
+        orig_dtype = x.dtype
+        x = x.to(torch.float32)
+        
+        variance = x.pow(2).mean(dim=-1, keepdim=True, dtype=torch.float32)
+        if self.tp_world > 1:
+            variance = tensor_model_parallel_all_reduce(
+                variance) / self.tp_world
+
+        x = x * torch.rsqrt(variance + self.variance_epsilon)
+        x = x.to(orig_dtype) * self.weight
+
+        return x
+
+class BailingLinearKernel:
+    """
+    Linear attention kernel implementation for Bailing models.
+    
+    This class is adapted from MiniMaxText01LinearKernel in vllm:
+    https://github.com/vllm-project/vllm/blob/a9138e85b14047e06300685b48e3485b995425fb/vllm/model_executor/models/minimax_text_01.py#L289
+    
+    The implementation maintains the same functionality while being renamed to
+    match our Bailing model naming convention.
+    """    
+    @staticmethod
+    def jit_linear_forward_prefix(q: torch.Tensor,
+                                k: torch.Tensor,
+                                v: torch.Tensor,
+                                kv_caches: torch.Tensor,
+                                slope_rate: torch.Tensor,
+                                block_size: int,
+                                layer_idx: int = None,
+                                **kwargs) -> torch.Tensor:
+
+        slope_rate = slope_rate.to(torch.float32)
+        should_pad_dim = q.dim() == 3
+        if should_pad_dim:
+            q = q.unsqueeze(0)
+            k = k.unsqueeze(0)
+            v = v.unsqueeze(0)
+        b, h, n, d = q.shape
+        e = d
+        kv_history = kv_caches.reshape(1, h, d, e).contiguous()
+        output, kv_history = lightning_attention2_parallel(
+            q, k, v, slope_rate, block_size=block_size, kv_history=kv_history)
+        
+        if kv_caches.numel() > 0 and kv_caches.shape == (h, d, e):
+            kv_caches.copy_(kv_history[:, :, -1, :, :].reshape(h, d, e))
+        
+        assert output.shape[0] == 1, "batch size must be 1"
+        return output.squeeze(0).transpose(0, 1).reshape(n, -1)
+
+class BailingLinearAttention(nn.Module):
+    """
+    Linear attention implementation for Bailing models.
+    
+    This class is partially inspired by MiniMaxText01LinearAttention from vllm:
+    https://github.com/vllm-project/vllm/blob/a9138e85b14047e06300685b48e3485b995425fb/vllm/model_executor/models/minimax_text_01.py#L321
+    
+    While the interface and structure are similar, we've made significant modifications
+    to the computation logic to better suit our specific requirements and performance
+    optimizations for the Bailing model architecture.
+    """
+
+    def __init__(
+        self,
+        config: BailingMoeLinearConfig,
+        hidden_size: int,
+        hidden_inner_size: int,
+        num_heads: int,
+        head_dim: int,
+        max_position: int,
+        block_size: int,
+        num_hidden_layer: int,
+        quant_config: Optional[QuantizationConfig] = None,
+        layer_idx: int = 0,
+        prefix: str = "linear_attn",
+    ) -> None:
+        super().__init__()
+
+        self.layer_idx = layer_idx
+        self.BLOCK = block_size
+        self.hidden_size = hidden_size
+        self.num_heads = num_heads
+        self.head_dim = head_dim
+        self.total_num_heads = num_heads
+
+        self.total_kv_heads = num_heads
+        self.hidden_inner_size = hidden_inner_size
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.tp_rank = get_tensor_model_parallel_rank()
+
+        assert self.total_num_heads % self.tp_size == 0
+        assert self.total_kv_heads % self.tp_size == 0
+
+        self.tp_heads = self.total_num_heads // self.tp_size
+        self.tp_kv_heads = self.total_kv_heads // self.tp_size
+
+        self.q_size_per_rank = self.head_dim * self.tp_heads
+        self.kv_size_per_rank = self.head_dim * self.tp_kv_heads
+
+        self.qkv_proj = QKVParallelLinear(
+            hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_kv_heads,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.qkv_proj",
+        )            
+
+        self.use_low_rank = config.use_low_rank       
+
+        if self.use_low_rank:
+            self.output_gate_down = ColumnParallelLinear(hidden_size, self.head_dim, bias=False, quant_config=quant_config, prefix = f"{prefix}.output_gate_down")
+            self.output_gate_up = RowParallelLinear(self.head_dim, self.total_num_heads * self.head_dim, bias=False, quant_config=quant_config, prefix=f"{prefix}.output_gate_up")
+        else:
+            self.output_gate = ColumnParallelLinear(
+                hidden_size,
+                self.hidden_inner_size,
+                bias=False,
+                quant_config=quant_config,
+                prefix=f"{prefix}.output_gate",
+            )
+        self.out_proj = RowParallelLinear(
+            self.hidden_inner_size,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.out_proj",
+        )
+
+        if self.tp_size == 1:
+            self.norm = RMSNorm(hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = BailingRMSNormTP(
+                self.hidden_inner_size,
+                eps=config.rms_norm_eps,
+            )
+
+        slope_rate = BailingLinearAttention._build_slope_tensor(self.num_heads)
+        self.slope_rate = slope_rate * (1 - layer_idx /
+                                        (num_hidden_layer - 1) + 1e-5)
+        self.tp_slope = self.slope_rate[self.tp_rank *
+                                        self.tp_heads:(self.tp_rank + 1) *
+                                        self.tp_heads].contiguous()
+        
+        self.use_linear_silu = config.use_linear_silu
+        self.linear_rope = config.linear_rope
+
+    @staticmethod
+    def weight_direct_load(param: torch.Tensor,
+                           loaded_weight: torch.Tensor) -> None:
+        assert param.size() == loaded_weight.size()
+        param.data.copy_(loaded_weight)
+        return
+
+    @staticmethod
+    def _build_slope_tensor(n_attention_heads: int):
+        def get_slopes(n):
+            def get_slopes_power_of_2(n):
+                start = 2**(-(2**-(math.log2(n) - 3)))
+                ratio = start
+                return [start * ratio**i for i in range(n)]
+
+            if math.log2(n).is_integer():
+                return get_slopes_power_of_2(n)
+            else:
+                closest_power_of_2 = 2**math.floor(math.log2(n))
+                return (get_slopes_power_of_2(closest_power_of_2) + get_slopes(
+                    2 * closest_power_of_2)[0::2][:n - closest_power_of_2])
+
+        slopes = torch.tensor(get_slopes(n_attention_heads),
+                              dtype=torch.float32).reshape(
+                                  n_attention_heads, 1, 1)
+        return slopes
+
+    @staticmethod
+    def weight2param_match(
+        model: nn.Module,
+        name: str,
+        all_params: Dict[str, torch.Tensor],
+    ) -> bool:
+        def which_layer(name: str) -> int:
+            if "layers" in name:
+                after_layer = name.split("layers")[-1]
+                return int(after_layer.split(".")[1])
+            return None
+        return is_linear_layer(which_layer(name), layer_group_size=model.config.layer_group_size)
+
+    @staticmethod
+    def weight2param_copy(
+        model: nn.Module,
+        name: str,
+        all_params: Dict[str, torch.Tensor],
+        loaded_weight: torch.Tensor,
+        prefix: str = "linear_attn",
+    ) -> None:
+        name = replace_weight_name(name, prefix=prefix)
+        if is_pp_missing_parameter(name, model):
+            return
+        param = all_params[name]
+        loader = getattr(param, "weight_loader", BailingLinearAttention.weight_direct_load)
+        loader = weight_loader_with_alias(name)(loader)
+        loader(param, loaded_weight)
+        return
+
+    def _prefill_and_mix_infer(self, q, k, v, kv_cache, state_indices_tensor,
+                               attn_metadata):
+        hidden = []
+        for _prefill_idx in range(attn_metadata.num_prefills):
+            _start = attn_metadata.query_start_loc[_prefill_idx]
+            _end = attn_metadata.query_start_loc[_prefill_idx + 1]
+            slot_id = state_indices_tensor[_prefill_idx]
+            qs = q[_start:_end].transpose(0, 1).contiguous()
+            ks = k[_start:_end].transpose(0, 1).contiguous()
+            vs = v[_start:_end].transpose(0, 1).contiguous()         
+            slot_id = state_indices_tensor[_prefill_idx]
+            slice_layer_cache = kv_cache[slot_id, ...]
+
+            out_slice = BailingLinearKernel.jit_linear_forward_prefix(
+                qs,
+                ks,
+                vs,
+                slice_layer_cache,
+                self.tp_slope,
+                self.BLOCK,
+                layer_idx=self.layer_idx)
+            hidden.append(out_slice.contiguous())
+        if attn_metadata.num_decode_tokens > 0:
+            hidden.append(
+                self._decode_infer(q, k, v, kv_cache, state_indices_tensor,
+                                   attn_metadata))
+        hidden = torch.concat(hidden, dim=0).contiguous()
+        return hidden
+
+    def _decode_infer(self, q, k, v, kv_cache, state_indices_tensor,
+                      attn_metadata):                   
+        q = q[attn_metadata.num_prefill_tokens:].unsqueeze(2).contiguous()
+        k = k[attn_metadata.num_prefill_tokens:].unsqueeze(2).contiguous()
+        v = v[attn_metadata.num_prefill_tokens:].unsqueeze(2).contiguous()  
+        slot_id = state_indices_tensor[attn_metadata.num_prefills:]
+        hidden = linear_decode_forward_triton(q, k, v, kv_cache, self.tp_slope,
+                                              slot_id, 32)
+        return hidden
+
+    def forward(
+            self,
+            hidden_states: torch.Tensor,
+            positions: torch.Tensor,
+            kv_caches: BailingCacheParams,
+            attn_metadata=None,
+            **kwargs) -> torch.Tensor:
+        
+        qkv, _ = self.qkv_proj(hidden_states)
+        if self.use_linear_silu:
+            qkv = torch.nn.functional.silu(qkv)
+
+        q, k, v = torch.split(
+            qkv,
+            [self.q_size_per_rank, self.kv_size_per_rank, self.kv_size_per_rank],
+            dim=-1
+        )
+        if self.linear_rope:
+            q, k = attn_metadata.rotary_emb(positions, q, k)
+
+        q = q.view((qkv.shape[0], self.tp_heads, self.head_dim))
+        k = k.view((qkv.shape[0], self.tp_kv_heads, self.head_dim))
+        v = v.view((qkv.shape[0], self.tp_kv_heads, self.head_dim))
+
+        forward_context = get_forward_context()
+        attn_metadata = forward_context.attn_metadata
+        kv_cache = kv_caches.bailing_cache
+        state_indices_tensor = kv_caches.state_indices_tensor
+        
+        scale = self.head_dim ** -0.5
+        q = q * scale
+
+        decode_only = attn_metadata.num_prefills == 0
+        if not decode_only:
+            # prefill and mix
+            hidden = self._prefill_and_mix_infer(q, k, v, kv_cache,
+                                                state_indices_tensor,
+                                                attn_metadata)
+        else:
+            # decode only
+            hidden = self._decode_infer(q, k, v, kv_cache,
+                                    state_indices_tensor, attn_metadata)
+
+        hidden = self.norm(hidden)
+
+        if self.use_low_rank:
+            output1,_ = self.output_gate_down(hidden_states)            
+            gate, _ = self.output_gate_up(output1)
+            if self.tp_size > 1:
+                shard_size = gate.size(-1) // self.tp_size
+                start_idx = self.tp_rank * shard_size
+                end_idx = start_idx + shard_size
+                gate = gate[..., start_idx:end_idx]                
+        else:
+            gate, _ = self.output_gate(hidden_states)
+
+        hidden = F.sigmoid(gate) * hidden
+        hidden = hidden.to(hidden_states.dtype)
+        hidden, _ = self.out_proj(hidden)
+        return hidden
+
+
+class BailingAttention(nn.Module):
+
+    def __init__(
+            self,
+            config: BailingMoeLinearConfig,
+            cache_config: Optional[CacheConfig] = None,
+            quant_config: Optional[QuantizationConfig] = None,
+            prefix: str = "",
+            layer_idx: int = None,
+    ):
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        self.total_num_heads = config.num_attention_heads
+        self.total_kv_heads = config.num_key_value_heads
+        self.layer_idx = layer_idx
+        tp_size = get_tensor_model_parallel_world_size()
+
+        assert self.total_num_heads % tp_size == 0
+        assert self.total_kv_heads % tp_size == 0
+        assert self.total_num_heads >= self.total_kv_heads
+
+        self.num_heads = self.total_num_heads // tp_size
+        self.head_dim = config.head_dim or (self.hidden_size // self.total_num_heads)
+        self.q_size_per_rank = self.head_dim * self.num_heads
+
+        self.num_kv_heads = self.total_kv_heads // tp_size
+        self.kv_size_per_rank = self.num_kv_heads * self.head_dim
+
+        self.scale = self.head_dim ** -0.5
+
+        self.query_key_value = QKVParallelLinear(
+            self.hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_kv_heads,
+            bias=(config.use_bias or config.use_qkv_bias),
+            quant_config=quant_config,
+            prefix=f"{prefix}.query_key_value",
+        )
+
+        self.dense = RowParallelLinear(self.total_num_heads * self.head_dim,
+                                       self.hidden_size,
+                                       bias=config.use_bias,
+                                       quant_config=quant_config,
+                                       prefix=f"{prefix}.dense",)
+
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scale,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              prefix=f"{prefix}.attn")
+
+
+    def forward(
+            self,
+            hidden_states: torch.Tensor,
+            position_ids: torch.Tensor,
+            kv_cache: KVCache,
+            attn_metadata: AttentionMetadata,
+    ) -> torch.Tensor:
+
+        qkv, _ = self.query_key_value(hidden_states)
+        q, k, v = qkv.split(
+            [self.q_size_per_rank, self.kv_size_per_rank, self.kv_size_per_rank],
+            dim=-1
+        )
+
+        q, k = attn_metadata.rotary_emb(position_ids, q, k)
+
+        context_layer = self.attn(
+            q,
+            k,
+            v,
+            kv_cache,
+            attn_metadata,
+        )
+
+        attn_output, _ = self.dense(context_layer)
+        return attn_output
+
+
+class BailingMLP(nn.Module):
+
+    def __init__(
+            self,
+            intermediate_size: int,
+            config: BailingMoeLinearConfig,
+            quant_config: Optional[QuantizationConfig] = None,
+            reduce_results: Optional[bool] = True,
+            prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            config.hidden_size, [intermediate_size] * 2,
+            bias=config.use_bias,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj",
+        )
+        self.down_proj = RowParallelLinear(
+            intermediate_size,
+            config.hidden_size,
+            bias=config.use_bias,
+            quant_config=quant_config,
+            reduce_results=reduce_results,
+            prefix=f"{prefix}.down_proj",
+        )
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x):
+        x, _ = self.gate_up_proj(x)
+        x = self.act_fn(x)
+        x, _ = self.down_proj(x)
+        return x
+
+class BailingMoE(nn.Module):
+
+    def __init__(
+            self,
+            intermediate_size: int,
+            config: BailingMoeLinearConfig,
+            quant_config: Optional[QuantizationConfig] = None,
+            reduce_results: Optional[bool] = True,
+            prefix: str = "",
+    ):
+        super().__init__()
+
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.tp_rank = get_tensor_model_parallel_rank()
+        self.num_experts = config.num_experts
+        self.top_k = config.num_experts_per_tok
+        self.norm_expert_prob = config.norm_topk_prob
+        self.hidden_size = config.hidden_size
+        self.quant_config = quant_config
+        self.num_shared_experts = config.num_shared_experts
+        # Gate always runs at half / full precision for now.
+        self.gate = ReplicatedLinear(self.hidden_size,
+                                     self.num_experts,
+                                     bias=False,
+                                     quant_config=None)
+
+        self.experts = FusedMoE(
+            num_experts=self.num_experts,
+            top_k=self.top_k,
+            hidden_size=self.hidden_size,
+            intermediate_size=config.moe_intermediate_size,
+            reduce_results=False,
+            renormalize=self.norm_expert_prob,
+            quant_config=quant_config,
+            prefix=f"{prefix}.experts"
+        )
+
+        if self.num_shared_experts > 0:
+            intermediate_size = (config.moe_intermediate_size *
+                                 self.num_shared_experts)
+            self.shared_experts = BailingMLP(
+                intermediate_size=intermediate_size,
+                config=config,
+                quant_config=quant_config,
+                reduce_results=False,
+                prefix=f"{prefix}.shared_experts"
+            )
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        num_tokens, hidden_size = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_size)
+        if self.num_shared_experts > 0:
+            shared_output = self.shared_experts(hidden_states)
+        # router_logits: (num_tokens, n_experts)
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.experts(
+            hidden_states=hidden_states, router_logits=router_logits
+        )
+
+        if self.num_shared_experts > 0:
+            final_hidden_states = final_hidden_states + shared_output
+
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+        return final_hidden_states.view(num_tokens, hidden_size)
+
+class BailingMoeLinearBlock(nn.Module):
+
+    def __init__(
+            self,
+            config: BailingMoeLinearConfig,
+            cache_config: Optional[CacheConfig] = None,
+            quant_config: Optional[QuantizationConfig] = None,
+            prefix: str = "",
+            layer_idx: int = None,
+    ):
+        super().__init__()
+        hidden_size = config.hidden_size
+        intermediate_size = config.intermediate_size
+        self.layer_idx = layer_idx
+        self.input_layernorm = RMSNorm(hidden_size, eps=config.rms_norm_eps)
+        self.layer_group_size = config.layer_group_size
+        head_dim = config.head_dim if config.head_dim else config.hidden_size // config.num_attention_heads
+
+        if (self.layer_idx + 1) % self.layer_group_size == 0:
+            self.attention = BailingAttention(config,
+                                      cache_config,
+                                      quant_config,
+                                      prefix=f"{prefix}.attention",
+                                      layer_idx=self.layer_idx)            
+        else:
+            if hasattr(config, "max_model_len") and isinstance(config.max_model_len, int):
+                max_position_embeddings = min(config.max_position_embeddings, config.max_model_len)    
+            else:
+                max_position_embeddings = 16384
+
+            self.attention = BailingLinearAttention(
+                config = config,
+                hidden_size=hidden_size,
+                hidden_inner_size=hidden_size,
+                num_heads=config.num_attention_heads,
+                head_dim=head_dim,
+                max_position=max_position_embeddings,
+                block_size=config.block if hasattr(config, "block") else 256,
+                num_hidden_layer=config.num_hidden_layers,
+                quant_config=quant_config,
+                layer_idx=self.layer_idx,
+                prefix=prefix + ".attention")
+
+        self.post_attention_layernorm = RMSNorm(hidden_size, eps=config.rms_norm_eps)
+        self.mlp = BailingMoE(intermediate_size, config, quant_config, True, prefix=f"{prefix}.mlp")
+
+    def forward(
+            self,
+            hidden_states: torch.Tensor,
+            position_ids: torch.Tensor,
+            kv_cache: KVCache,
+            attn_metadata: AttentionMetadata,
+            residual: Optional[torch.Tensor],
+    ) -> torch.Tensor:
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+
+        if isinstance(self.attention, BailingLinearAttention):
+            hidden_states = self.attention(
+                hidden_states=hidden_states,
+                positions=position_ids,
+                kv_caches=kv_cache,
+                attn_metadata=attn_metadata
+            )
+        else:
+            hidden_states = self.attention(
+                hidden_states=hidden_states,
+                position_ids=position_ids,
+                kv_cache=kv_cache,
+                attn_metadata=attn_metadata
+            )
+
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+def make_linear_layers(
+    num_hidden_layers: int,
+    layer_fn: Callable[[int, str], nn.Module],
+    prefix: str,
+) -> Tuple[int, int, nn.ModuleList]:
+    from vllm.distributed.parallel_state import get_pp_group
+    from vllm.distributed.utils import get_pp_indices
+    from vllm.model_executor.models.utils import maybe_offload_to_cpu, PPMissingLayer
+    
+    start_layer, end_layer = get_pp_indices(num_hidden_layers,
+                                           get_pp_group().rank_in_group,
+                                           get_pp_group().world_size)
+    modules = nn.ModuleList(
+        [PPMissingLayer() for _ in range(start_layer)] + [
+            maybe_offload_to_cpu(layer_fn(idx, f"{prefix}.{idx}"))
+            for idx in range(start_layer, end_layer)
+        ] + [PPMissingLayer() for _ in range(end_layer, num_hidden_layers)])
+    return start_layer, end_layer, modules
+
+class BailingMoeLinearModel(nn.Module):
+
+    def __init__(
+            self,
+            *, 
+            vllm_config: VllmConfig,
+            prefix: str = "",
+    ):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        self.config = config
+        self.vocab_size = config.vocab_size
+        self.embed_dim = config.hidden_size
+
+        if get_pp_group().is_first_rank or (config.tie_word_embeddings
+                                            and get_pp_group().is_last_rank):
+            self.word_embeddings = VocabParallelEmbedding(self.vocab_size, self.embed_dim)
+        else:
+            self.word_embeddings = PPMissingLayer()
+
+        self.embedding_dropout = torch.nn.Dropout(config.embedding_dropout)
+
+        self.start_layer, self.end_layer, self.layers = make_linear_layers(
+            config.num_hidden_layers,
+            lambda layer_idx, prefix: BailingMoeLinearBlock(
+                config=config,
+                cache_config=cache_config,
+                quant_config=quant_config,
+                prefix=prefix,
+                layer_idx=layer_idx,  # 传递 layer_idx 参数
+            ),
+            prefix=f"{prefix}.layers"
+        )
+        max_slots_number = vllm_config.scheduler_config.max_num_seqs
+        self.layer_group_size = config.layer_group_size
+
+        # config.layer_group_size should divide config.num_hidden_layers
+        assert config.num_hidden_layers % config.layer_group_size == 0, f"config.num_hidden_layers % config.layer_group_size != 0"
+
+        head_dim = config.head_dim if config.head_dim else config.hidden_size // config.num_attention_heads
+        linear_layer_nums = config.num_hidden_layers - config.num_hidden_layers // self.layer_group_size
+        self.cache_shape = (linear_layer_nums, max_slots_number,
+                            config.num_attention_heads //
+                            get_tensor_model_parallel_world_size(),
+                            head_dim, head_dim)
+        _dummy = torch.zeros(1)
+        self._dtype = _dummy.dtype
+        del _dummy
+        self.bailing_cache = BailingCacheManager(dtype=self._dtype, cache_shape=self.cache_shape)
+
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size
+            )
+        )
+        self.rotary_type = config.rotary_type
+
+        full_1d = bool(self.rotary_type == 'full-1d')
+        rotary_dim = head_dim if full_1d else (head_dim // 2)
+        self.rotary_emb = get_rope(
+            head_dim,
+            rotary_dim=rotary_dim,
+            max_position=config.max_position_embeddings,
+            base=config.rope_theta,
+            is_neox_style=True,
+            rope_scaling=config.rope_scaling,
+        )
+
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(self.embed_dim, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+
+        # calculate the number of gqa layers, for applying standard layer kv cache
+        config.num_hidden_layers = config.num_hidden_layers // self.layer_group_size
+    
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.word_embeddings(input_ids)
+
+    def _clear_prefill_cache(self, attn_metadata,
+                             bailing_cache_tensors: torch.Tensor, **kwargs):
+        """
+        clear the bailing cache before new prefill requests computing
+        """
+        seq_to_slot_maps = {}
+        seq_id_map = sum(list(kwargs["request_ids_to_seq_ids"].values()), [])
+        for _, seq_to_slot_map in (
+                self.bailing_cache.cache_indices_mapping.items()):
+            seq_to_slot_maps.update(seq_to_slot_map)
+
+        slots_to_clear = []
+        for _prefill_id in range(getattr(attn_metadata, "num_prefills", 0)):
+            seq_id = seq_id_map[_prefill_id]
+            if attn_metadata.context_lens_tensor[
+                    _prefill_id] == 0 and seq_id in seq_to_slot_maps:
+                slots_to_clear.append(seq_to_slot_maps[seq_id])
+
+        if slots_to_clear:
+            slots_tensor = torch.tensor(slots_to_clear,
+                                        device=bailing_cache_tensors.device,
+                                        dtype=torch.long)
+            bailing_cache_tensors[:, slots_tensor, ...] = 0       
+
+    def forward(
+            self,
+            input_ids: torch.Tensor,
+            position_ids: torch.Tensor,
+            kv_caches: List[KVCache],
+            attn_metadata: AttentionMetadata,
+            intermediate_tensors: Optional[IntermediateTensors],
+            inputs_embeds: Optional[torch.Tensor] = None,
+            **kwargs,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        forward_context = get_forward_context()
+        attn_metadata = forward_context.attn_metadata
+        if attn_metadata is None:
+            return None
+        if "request_ids_to_seq_ids" not in kwargs:
+            kwargs["request_ids_to_seq_ids"] = {}
+        if "finished_requests_ids" not in kwargs:
+            kwargs["finished_requests_ids"] = []        
+        (
+            bailing_cache_tensors,
+            state_indices_tensor,
+        ) = self.bailing_cache.current_run_tensors(input_ids, attn_metadata, **kwargs)
+        if attn_metadata.num_prefills > 0:
+            self._clear_prefill_cache(attn_metadata, bailing_cache_tensors, **kwargs)
+        bailing_cache_params = BailingCacheParams(bailing_cache_tensors, state_indices_tensor)
+        
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+
+        kv_cache_index = 0
+        bailing_cache_index = 0    
+        attn_metadata.rotary_emb = self.rotary_emb
+
+        for i in range(self.start_layer, self.end_layer):
+            layer = self.layers[i]
+            _caches = None
+            if isinstance(layer.attention, BailingAttention):
+                _caches = kv_caches[kv_cache_index]
+                kv_cache_index += 1
+            if isinstance(layer.attention, BailingLinearAttention):
+                current_state_layer = bailing_cache_index
+                _caches = bailing_cache_params.at_layer_idx(
+                    current_state_layer)
+                bailing_cache_index += 1     
+
+            hidden_states, residual = layer(
+                hidden_states,
+                position_ids,
+                _caches,
+                attn_metadata,
+                residual
+            )
+
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+class BailingMoeLinearForCausalLM(nn.Module, SupportsLoRA, SupportsPP, HasInnerState):
+
+    packed_modules_mapping = {
+        "query_key_value": ["query_key_value"],
+        "dense_h_to_4h": ["dense_h_to_4h"],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    # LoRA specific attributes
+    supported_lora_modules = [
+        "query_key_value",
+        "dense",
+        "dense_h_to_4h",
+        "dense_4h_to_h",
+        "gate_up_proj",
+        "down_proj",
+    ]
+    embedding_modules = {}
+    embedding_padding_modules = []
+
+    def __init__(
+            self,
+            *,
+            vllm_config: VllmConfig,
+            prefix: str = "",
+    ) -> None:
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.lora_config = lora_config
+        self.quant_config = quant_config
+        self.max_position_embeddings = config.max_position_embeddings
+        self.model = BailingMoeLinearModel(
+                                    vllm_config=vllm_config,
+                                    prefix=maybe_prefix(prefix, "model")
+        )
+        if get_pp_group().is_last_rank:
+            self.lm_head = self.word_embeddings if config.tie_word_embeddings \
+                else ParallelLMHead(config.vocab_size, config.hidden_size, quant_config=quant_config)
+            self.logits_processor = LogitsProcessor(config.vocab_size)
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.sampler = get_sampler()
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors
+        )
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def copy_inputs_before_cuda_graphs(self, input_buffers, **kwargs):
+        return self.model.bailing_cache.copy_inputs_before_cuda_graphs(input_buffers, **kwargs)
+
+    def get_seqlen_agnostic_capture_inputs(self, batch_size: int):
+        return self.model.bailing_cache.get_seqlen_agnostic_capture_inputs(batch_size)
+
+    def forward(
+            self,
+            input_ids: torch.Tensor,
+            positions: torch.Tensor,
+            kv_caches: List[KVCache],
+            attn_metadata: AttentionMetadata,
+            intermediate_tensors: Optional[IntermediateTensors] = None,
+            inputs_embeds: Optional[torch.Tensor] = None,
+            **kwargs,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        model_output = self.model(input_ids, positions, kv_caches,
+                                         attn_metadata, intermediate_tensors,
+                                         inputs_embeds, **kwargs)
+        return model_output
+
+    def compute_logits(
+            self,
+            hidden_states: torch.Tensor,
+            sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states, sampling_metadata)
+        return logits
+
+    def sample(
+            self,
+            logits: torch.Tensor,
+            sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]) -> Set[str]:
+        def load_linear_attn_weight(name: str, loaded_weight: torch.Tensor, self) -> None:
+            if is_pp_missing_parameter(name, self):
+                return
+            param = params_dict[name]
+            weight_loader = getattr(
+                param, "weight_loader",
+                BailingLinearAttention.weight_direct_load)
+            weight_loader = weight_loader_with_alias(name)(weight_loader)
+            weight_loader(param, loaded_weight)
+            return    
+
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.num_experts)
+
+        params_dict = dict(self.named_parameters(remove_duplicate=False))
+        loaded_params: Set[str] = set()
+
+        for name, loaded_weight in weights:
+            layer_idx = None
+            if 'model.layers.' in name:
+                layer_idx = int(name.split('.')[2])
+            if (("v_head" in name) or ("inv_freq" in name) or
+                    (self.config.tie_word_embeddings and "lm_head" in name)):
+                continue
+            if self.config.norm_head and "lm_head.weight" in name:
+                import torch.nn.functional as F
+                weight_norm = loaded_weight.norm(dim=0, keepdim=True, p=2)
+                loaded_weight = loaded_weight / (weight_norm + 1e-7)                
+
+            if "attention" in name:
+                if layer_idx is not None and is_linear_layer(layer_idx):
+                    if "query_key_value.weight" in name:
+                        name = name.replace("query_key_value.weight", "qkv_proj.weight")
+                    elif "o_proj.weight" in name:
+                        name = name.replace("o_proj.weight", "out_proj.weight")
+                    elif "g_proj.0.weight" in name:
+                        name = name.replace("g_proj.0.weight", "output_gate_down.weight")
+                    elif "g_proj.1.weight" in name:
+                        name = name.replace("g_proj.1.weight", "output_gate_up.weight")
+                    elif "g_proj.weight" in name:
+                        name = name.replace("g_proj.weight", "output_gate.weight")
+                    elif "g_norm.weight" in name:
+                        name = name.replace("g_norm.weight", "norm.weight")
+                    elif "attention.dense.weight" in name:
+                        name = name.replace("attention.dense.weight", "attention.out_proj.weight")
+                        
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                if "mlp.experts" in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                loaded_weight,
+                                name,
+                                shard_id=shard_id,
+                                expert_id=expert_id)
+                    break
+                else:
+                    if name.endswith(".bias") and name not in params_dict:
+                        continue
+                    if 'slope' in name:
+                        continue
+                    if "attention" in name and 'slope' not in name and is_linear_layer(layer_idx):
+                        load_linear_attn_weight(name, loaded_weight, self)
+                        loaded_params.add(name)
+                        continue
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader", default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
diff --git a/vllm/model_executor/models/constant_size_cache.py b/vllm/model_executor/models/constant_size_cache.py
new file mode 100644
index 000000000..e03896421
--- /dev/null
+++ b/vllm/model_executor/models/constant_size_cache.py
@@ -0,0 +1,145 @@
+# SPDX-License-Identifier: Apache-2.0
+#
+# This file is directly imported from vllm without modifications:
+# https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/constant_size_cache.py
+#
+# The original implementation is used as-is to maintain compatibility with the vllm ecosystem.
+
+from abc import ABC, abstractmethod
+from typing import Any, Dict, List, Tuple
+
+import torch
+
+from vllm.attention.backends.abstract import AttentionMetadata
+from vllm.attention.backends.utils import PAD_SLOT_ID
+
+
+class ConstantSizeCache(ABC):
+    """
+    Abstract base class for managing constant size caches 
+    like Mamba and Minimax.
+    """
+
+    def __init__(self, max_batch_size: int):
+        # Maps between the request id and a dict that maps between the seq_id
+        # and its index inside the cache
+        self.cache_indices_mapping: Dict[str, Dict[int, int]] = {}
+        self.free_cache_indices = list(range(max_batch_size))
+
+    @property
+    @abstractmethod
+    def cache(self) -> Any:
+        """Return the underlying cache tensor(s)"""
+        pass
+
+    @abstractmethod
+    def _copy_cache(self, from_index: int, to_index: int):
+        """Copy cache data from one index to another"""
+        pass
+
+    def current_run_tensors(self, input_ids: torch.Tensor,
+                            attn_metadata: AttentionMetadata,
+                            **kwargs) -> Tuple:
+        """
+        Return the tensors for the current run's conv and ssm state.
+        """
+        if "seqlen_agnostic_capture_inputs" not in kwargs:
+            # We get here only on Prefill/Eager mode runs
+            request_ids_to_seq_ids = kwargs["request_ids_to_seq_ids"]
+            finished_requests_ids = kwargs["finished_requests_ids"]
+
+            self._release_finished_requests(finished_requests_ids)
+            state_indices = self._prepare_current_run_cache(
+                request_ids_to_seq_ids, finished_requests_ids)
+
+            state_indices_tensor = torch.as_tensor(state_indices,
+                                                   dtype=torch.int32,
+                                                   device="cuda")
+            cache_tensors = self.cache
+        else:
+            # CUDA graph capturing runs
+            cache_tensors, state_indices_tensor = kwargs[
+                "seqlen_agnostic_capture_inputs"]
+
+        return (cache_tensors, state_indices_tensor)
+
+    def copy_inputs_before_cuda_graphs(self, input_buffers, **kwargs):
+        """
+        Copy the relevant state_indices into the CUDA graph input buffer 
+        """
+        assert all(
+            key in kwargs
+            for key in ["request_ids_to_seq_ids", "finished_requests_ids"])
+        finished_requests_ids = kwargs["finished_requests_ids"]
+        request_ids_to_seq_ids = kwargs["request_ids_to_seq_ids"]
+        assert "seqlen_agnostic_capture_inputs" in input_buffers
+        _, input_state_indices_buffer = input_buffers[
+            "seqlen_agnostic_capture_inputs"]
+
+        self._release_finished_requests(finished_requests_ids)
+        state_indices = self._prepare_current_run_cache(
+            request_ids_to_seq_ids, finished_requests_ids)
+        cuda_graph_pad_len = input_state_indices_buffer.shape[0] - len(
+            state_indices)
+        state_indices.extend([PAD_SLOT_ID] * cuda_graph_pad_len)
+
+        input_state_indices_buffer.copy_(
+            torch.as_tensor(state_indices, dtype=torch.int32, device="cuda"))
+
+    def get_seqlen_agnostic_capture_inputs(self, batch_size: int):
+        """
+        Provide the CUDA graph capture runs with a buffer in adjusted size.
+        The buffer is used to maintain the Cache during the CUDA graph replay
+        runs.
+        """
+        state_indices_tensor = torch.as_tensor([PAD_SLOT_ID] * batch_size,
+                                               dtype=torch.int32,
+                                               device="cuda")
+        return (self.cache, state_indices_tensor)
+
+    def _assign_seq_id_to_cache_index(self, cur_rid: str, seq_id: int,
+                                      finished_requests_ids) -> int:
+        """
+        Assign (req_id,seq_id) pair to a `destination_index` index, if
+        already occupied, move the occupying index to a free index.
+        """
+        if cur_rid in finished_requests_ids:
+            # set as pad, do not allocate destination index
+            return PAD_SLOT_ID
+        elif cur_rid not in self.cache_indices_mapping:
+            destination_index = self.free_cache_indices.pop()
+            self.cache_indices_mapping[cur_rid] = {seq_id: destination_index}
+            return destination_index
+        elif seq_id not in (seq_ids2indices :=
+                            self.cache_indices_mapping[cur_rid]):
+            # parallel sampling , where n > 1, assume prefill have
+            # already happened, so we copy the
+            # existing cache into the siblings seq_ids caches
+            index_exists = next(iter(seq_ids2indices.values()))
+            # case of decoding n>1, copy prefill cache to decoding indices
+            destination_index = self.free_cache_indices.pop()
+            self._copy_cache(from_index=index_exists,
+                             to_index=destination_index)
+            self.cache_indices_mapping[cur_rid][seq_id] = destination_index
+            return destination_index
+        else:
+            return self.cache_indices_mapping[cur_rid][seq_id]
+
+    def _prepare_current_run_cache(
+            self, request_ids_to_seq_ids: Dict[str, list[int]],
+            finished_requests_ids: List[str]) -> List[int]:
+        return [
+            self._assign_seq_id_to_cache_index(req_id, seq_id,
+                                               finished_requests_ids)
+            for req_id, seq_ids in request_ids_to_seq_ids.items()
+            for seq_id in seq_ids
+        ]
+
+    def _release_finished_requests(self,
+                                   finished_seq_groups_req_ids: List[str]):
+        for req_id in finished_seq_groups_req_ids:
+            if req_id in self.cache_indices_mapping:
+                for seq_id in self.cache_indices_mapping[req_id]:
+                    self.free_cache_indices.append(
+                        self.cache_indices_mapping[req_id][seq_id])
+                self.cache_indices_mapping.pop(req_id)
diff --git a/vllm/model_executor/models/registry.py b/vllm/model_executor/models/registry.py
index 81623defd..f61f35503 100644
--- a/vllm/model_executor/models/registry.py
+++ b/vllm/model_executor/models/registry.py
@@ -39,6 +39,8 @@ _TEXT_GENERATION_MODELS = {
     "BaichuanForCausalLM": ("baichuan", "BaichuanForCausalLM"),
     "BambaForCausalLM": ("bamba", "BambaForCausalLM"),
     "BloomForCausalLM": ("bloom", "BloomForCausalLM"),
+    "BailingMoeForCausalLM": ("bailing_moe", "BailingMoeForCausalLM"),
+    "BailingMoeLinearForCausalLM": ("bailing_moe_linear", "BailingMoeLinearForCausalLM"),
     "ChatGLMModel": ("chatglm", "ChatGLMForCausalLM"),
     "CohereForCausalLM": ("commandr", "CohereForCausalLM"),
     "Cohere2ForCausalLM": ("commandr", "CohereForCausalLM"),
diff --git a/vllm/transformers_utils/configs/__init__.py b/vllm/transformers_utils/configs/__init__.py
index 906056559..942a1b91c 100644
--- a/vllm/transformers_utils/configs/__init__.py
+++ b/vllm/transformers_utils/configs/__init__.py
@@ -23,6 +23,8 @@ from vllm.transformers_utils.configs.olmo2 import Olmo2Config
 from vllm.transformers_utils.configs.solar import SolarConfig
 from vllm.transformers_utils.configs.telechat2 import Telechat2Config
 from vllm.transformers_utils.configs.ultravox import UltravoxConfig
+from vllm.transformers_utils.configs.bailing_moe import BailingMoeConfig
+from vllm.transformers_utils.configs.bailing_moe_linear import BailingMoeLinearConfig
 
 __all__ = [
     "ChatGLMConfig",
@@ -45,4 +47,6 @@ __all__ = [
     "SolarConfig",
     "Telechat2Config",
     "UltravoxConfig",
+    "BailingMoeConfig",
+    "BailingMoeLinearConfig",
 ]
diff --git a/vllm/transformers_utils/configs/bailing_moe.py b/vllm/transformers_utils/configs/bailing_moe.py
new file mode 100644
index 000000000..4379368cf
--- /dev/null
+++ b/vllm/transformers_utils/configs/bailing_moe.py
@@ -0,0 +1,76 @@
+""" Bailing MoE model configuration """
+
+from transformers.configuration_utils import PretrainedConfig
+
+
+class BailingMoeConfig(PretrainedConfig):
+    model_type = "bailing_moe"
+
+    def __init__(
+        self,
+        vocab_size=30592,
+        hidden_size=1024,
+        intermediate_size=None,
+        num_hidden_layers=24,
+        num_attention_heads=16,
+        num_key_value_heads=0,
+        hidden_act="silu",
+        use_qkv_bias=False,  # bailing only
+        use_bias=True,  # bailing only
+        rms_norm_eps=1e-05,
+        norm_head=False,  # bailing only
+        tie_word_embeddings=False,  # PretrainedConfig key, here change default value.
+        embedding_dropout=0.1,
+        attention_dropout=0.1,
+        output_dropout=0.1,
+        initializer_range=0.02,
+        max_position_embeddings=16384,
+        rope_theta=10000.0,
+        use_cache=True,
+        use_sliding_window=False,
+        sliding_window=4096,
+        max_window_layers=28,
+        rope_scaling=None,
+        pad_token_id=126081,
+        num_experts=16,
+        num_shared_experts=0,
+        num_experts_per_tok=2,
+        norm_topk_prob=True,
+        moe_intermediate_size=None,
+        first_k_dense_replace=0,
+        head_dim=None,
+        **kwargs,
+    ):
+        self.num_hidden_layers = num_hidden_layers
+        self.vocab_size = vocab_size
+        self.hidden_size = hidden_size
+        self.intermediate_size = intermediate_size
+        self.num_attention_heads = num_attention_heads
+        self.num_key_value_heads = num_key_value_heads
+        self.hidden_act = hidden_act
+        self.use_qkv_bias = use_qkv_bias
+        self.use_bias = use_bias
+        self.norm_head = norm_head
+        self.rms_norm_eps = rms_norm_eps
+        self.embedding_dropout = embedding_dropout
+        self.attention_dropout = attention_dropout
+        self.output_dropout = output_dropout
+        self.initializer_range = initializer_range
+        self.max_position_embeddings = max_position_embeddings
+        self.rope_theta = rope_theta
+        self.use_cache = use_cache
+        self.use_sliding_window = use_sliding_window
+        self.sliding_window = sliding_window
+        self.max_window_layers = max_window_layers
+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads
+        self.rope_scaling = rope_scaling
+
+        # MoE configs
+        self.num_experts = num_experts
+        self.num_shared_experts = num_shared_experts
+        self.num_experts_per_tok = num_experts_per_tok
+        self.norm_topk_prob = norm_topk_prob
+        self.moe_intermediate_size = moe_intermediate_size
+        self.first_k_dense_replace = first_k_dense_replace
+
+        super().__init__(pad_token_id=pad_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs)
diff --git a/vllm/transformers_utils/configs/bailing_moe_linear.py b/vllm/transformers_utils/configs/bailing_moe_linear.py
new file mode 100644
index 000000000..bdd35576d
--- /dev/null
+++ b/vllm/transformers_utils/configs/bailing_moe_linear.py
@@ -0,0 +1,92 @@
+""" Bailing MoE Linear model configuration """
+
+from transformers.configuration_utils import PretrainedConfig
+
+
+class BailingMoeLinearConfig(PretrainedConfig):
+    model_type = "bailing_moe_linear"
+
+    def __init__(
+        self,
+        vocab_size=30592,
+        hidden_size=1024,
+        intermediate_size=None,
+        num_hidden_layers=24,
+        num_attention_heads=16,
+        num_key_value_heads=0,
+        hidden_act="silu",
+        use_qkv_bias=False,  # bailing only
+        use_bias=True,  # bailing only
+        rms_norm_eps=1e-05,
+        norm_head=False,  # bailing only
+        tie_word_embeddings=False,  # PretrainedConfig key, here change default value.
+        embedding_dropout=0.1,
+        attention_dropout=0.1,
+        output_dropout=0.1,
+        initializer_range=0.02,
+        max_position_embeddings=16384,
+        rope_theta=10000.0,
+        use_cache=True,
+        use_sliding_window=False,
+        sliding_window=4096,
+        max_window_layers=28,
+        rope_scaling=None,
+        pad_token_id=126081,
+        num_experts=16,
+        num_shared_experts=0,
+        num_experts_per_tok=2,
+        norm_topk_prob=True,
+        moe_intermediate_size=None,
+        first_k_dense_replace=0,
+        head_dim=None,
+        layer_group_size=1,
+        use_linear_silu=False,
+        linear_rope=True,
+        use_linear_gqa=False,
+        use_low_rank=False,
+        rotary_type='full-1d',
+        linear_mode='chunk',
+        **kwargs,
+    ):
+        self.num_hidden_layers = num_hidden_layers
+        self.vocab_size = vocab_size
+        self.hidden_size = hidden_size
+        self.intermediate_size = intermediate_size
+        self.num_attention_heads = num_attention_heads
+        self.num_key_value_heads = num_key_value_heads
+        self.hidden_act = hidden_act
+        self.use_qkv_bias = use_qkv_bias
+        self.use_bias = use_bias
+        self.norm_head = norm_head
+        self.rms_norm_eps = rms_norm_eps
+        self.embedding_dropout = embedding_dropout
+        self.attention_dropout = attention_dropout
+        self.output_dropout = output_dropout
+        self.initializer_range = initializer_range
+        self.max_position_embeddings = max_position_embeddings
+        self.rope_theta = rope_theta
+        self.use_cache = use_cache
+        self.use_sliding_window = use_sliding_window
+        self.sliding_window = sliding_window
+        self.max_window_layers = max_window_layers
+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads
+        self.rope_scaling = rope_scaling
+
+        # MoE configs
+        self.num_experts = num_experts
+        self.num_shared_experts = num_shared_experts
+        self.num_experts_per_tok = num_experts_per_tok
+        self.norm_topk_prob = norm_topk_prob
+        self.moe_intermediate_size = moe_intermediate_size
+        self.first_k_dense_replace = first_k_dense_replace
+
+        # hybrid linear configs
+        self.layer_group_size = layer_group_size
+        self.use_linear_silu = use_linear_silu
+        self.linear_rope = linear_rope
+        self.use_linear_gqa = use_linear_gqa
+        self.use_low_rank = use_low_rank
+        self.rotary_type = rotary_type
+        self.linear_mode = linear_mode
+        
+        super().__init__(pad_token_id=pad_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs)
